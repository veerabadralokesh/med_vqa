{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e085d48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/ocean/projects/asc170022p/mtragoza/med_vqa'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a521b039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import torch\n",
    "import open_clip\n",
    "import transformers as T\n",
    "import datasets\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b50db76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA RTX A6000\n",
      "NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_properties(i).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e5af9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.04 s, sys: 1.36 s, total: 8.4 s\n",
      "Wall time: 4.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class ImageEncoder(torch.nn.Module):\n",
    "    \n",
    "    @classmethod\n",
    "    def from_name(cls, name, **kwargs):\n",
    "        if name == 'CLIP':\n",
    "            url = 'laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K' # (256, 1024)\n",
    "        elif name == 'PMC-CLIP':\n",
    "            url = 'ryanyip7777/pmc_vit_l_14'\n",
    "\n",
    "        model, train_preprocess, val_preprocess = \\\n",
    "            open_clip.create_model_and_transforms(f'hf-hub:{url}', device=torch.device('cuda'))\n",
    "    \n",
    "        return cls(model.visual, train_preprocess, val_preprocess, n_patches=256, embed_size=1024, **kwargs)\n",
    "\n",
    "    def __init__(self, model, train_preprocess, val_preprocess, n_patches, embed_size, embed_type):\n",
    "        super().__init__()\n",
    "\n",
    "        self.train_preprocess = train_preprocess\n",
    "        self.val_preprocess = val_preprocess\n",
    "\n",
    "        self.model = model\n",
    "        self.model.output_tokens = True\n",
    "        self.model.proj = None\n",
    "    \n",
    "        self.embed_type = embed_type\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        if embed_type == 'both':\n",
    "            self.seq_length = n_patches + 1\n",
    "        elif embed_type == 'patch':\n",
    "            self.seq_length = n_patches\n",
    "        elif embed_type == 'global':\n",
    "            self.seq_length = 1\n",
    "        \n",
    "    def forward(self, images):\n",
    "\n",
    "        global_embeddings, patch_embeddings = self.model(images)\n",
    "        global_embeddings = global_embeddings.unsqueeze(1)\n",
    "        \n",
    "        if self.embed_type == 'global':\n",
    "            image_embeddings = global_embeddings\n",
    "        elif self.embed_type == 'patch':\n",
    "            image_embeddings = patch_embeddings  \n",
    "        elif self.embed_type == 'both':\n",
    "            image_embeddings = torch.cat([global_embeddings, patch_embeddings], dim=1)\n",
    "        \n",
    "        assert image_embeddings.shape[1:] == (self.seq_length, self.embed_size), image_embeddings.shape\n",
    "        return image_embeddings\n",
    "\n",
    "image_encoder = ImageEncoder.from_name('PMC-CLIP', embed_type='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "228a27dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85fb2a9024024cb7977d0e2e9e47c736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.87 s, sys: 7.43 s, total: 13.3 s\n",
      "Wall time: 12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class TextDecoder(torch.nn.Module):\n",
    "    \n",
    "    @classmethod\n",
    "    def from_name(cls, name):\n",
    "        if name == 'LLaMA':\n",
    "            url = 'meta-llama/Llama-2-7b-hf'\n",
    "        elif name == 'PMC-LLaMA':\n",
    "            url = 'chaoyi-wu/PMC_LLAMA_7B'\n",
    "        \n",
    "        model = T.LlamaForCausalLM.from_pretrained(url, device_map='auto')\n",
    "        tokenizer = T.LlamaTokenizer.from_pretrained(url)\n",
    "        \n",
    "        return cls(model, tokenizer, max_length=512, embed_size=4096)\n",
    "\n",
    "    def __init__(self, model, tokenizer, max_length, embed_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "    def forward(self, input_embeddings, mask, labels):\n",
    "        assert input_embeddings.shape[1] <= self.max_length, input_embeddings.shape\n",
    "        assert input_embeddings.shape[2] == self.embed_size, input_embeddings.shape\n",
    "\n",
    "        return self.model.forward(\n",
    "            inputs_embeds=input_embeddings,\n",
    "            attention_mask=mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "    def generate(self, input_embeddings, mask, **kwargs):\n",
    "        assert input_embeddings.shape[1] <= self.max_length, input_embeddings.shape\n",
    "        assert input_embeddings.shape[2] == self.embed_size, input_embeddings.shape\n",
    "\n",
    "        return self.model.generate(\n",
    "            inputs_embeds=input_embeddings,\n",
    "            attention_mask=mask,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "text_decoder = TextDecoder.from_name('PMC-LLaMA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f837061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalFusion(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, image_embed_size, text_embed_size, device):\n",
    "        super().__init__()\n",
    "        self.image_embed_size = image_embed_size\n",
    "        self.text_embed_size = text_embed_size\n",
    "\n",
    "        self.project_image = torch.nn.Linear(image_embed_size, text_embed_size, device=device)\n",
    "        \n",
    "    def forward(self, image_embeddings, text_embeddings, mask):\n",
    "        batch_size, image_length = image_embeddings.shape[:2]\n",
    "\n",
    "        image_embeddings = self.project_image(image_embeddings)\n",
    "        combined_embeddings = torch.cat([image_embeddings, text_embeddings], dim=1)\n",
    "\n",
    "        image_mask = torch.ones((batch_size, image_length), device=mask.device)\n",
    "        combined_mask = torch.cat([image_mask, mask], dim=1)\n",
    "\n",
    "        return combined_embeddings, combined_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71dac54f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class VQAModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, image_encoder, text_encoder, text_decoder):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder  = text_encoder\n",
    "        self.text_decoder  = text_decoder\n",
    "\n",
    "        self.fusion_module = MultimodalFusion(\n",
    "            image_encoder.embed_size,\n",
    "            text_decoder.embed_size,\n",
    "            device=text_decoder.model.device\n",
    "        )\n",
    "        \n",
    "    def combine_multimodal_inputs(self, images, padded_tokens, mask):   \n",
    "        image_embeddings = self.image_encoder(images)\n",
    "        text_embeddings  = self.text_encoder(padded_tokens)\n",
    "\n",
    "        combined_embeddings, combined_mask = self.fusion_module(\n",
    "            image_embeddings, text_embeddings, mask\n",
    "        )\n",
    "        return combined_embeddings, combined_mask\n",
    "    \n",
    "    def forward(self, images, padded_tokens, mask):       \n",
    "        input_embeddings, mask = self.combine_multimodal_inputs(images, padded_tokens, mask)\n",
    "        \n",
    "        dummy_tokens = torch.zeros(\n",
    "            (images.shape[0], self.image_encoder.seq_length),\n",
    "            dtype=padded_tokens.dtype,\n",
    "            device=padded_tokens.device\n",
    "        )\n",
    "        labels = torch.cat([dummy_tokens, padded_tokens], dim=1)\n",
    "        \n",
    "        output = self.text_decoder.forward(input_embeddings, mask, labels)\n",
    "        return output\n",
    "\n",
    "    def generate(self, images, padded_tokens, mask, **kwargs):\n",
    "        input_embeddings, mask = self.combine_multimodal_inputs(images, padded_tokens, mask)\n",
    "        output = self.text_decoder.generate(input_embeddings, mask, **kwargs)\n",
    "        return output\n",
    "\n",
    "\n",
    "model = VQAModel(\n",
    "    image_encoder=image_encoder,\n",
    "    text_encoder=text_decoder.model.model.embed_tokens,\n",
    "    text_decoder=text_decoder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "413525cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224]) torch.float32 cuda:0\n",
      "torch.Size([255]) torch.int64 cuda:0\n",
      "torch.Size([255]) torch.int64 cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_30359/3239635773.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(image, device=self.device),\n"
     ]
    }
   ],
   "source": [
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    @classmethod\n",
    "    def from_name(cls, name, train_preprocess, val_preprocess, **kwargs):\n",
    "\n",
    "        if name == 'VQA-RAD':\n",
    "            url = 'flaviagiammarino/vqa-rad'\n",
    "            val_split = 'test'\n",
    "        elif name == 'SLAKE':\n",
    "            url = 'BoKelvin/SLAKE'\n",
    "            val_split = 'validation'\n",
    "\n",
    "        ds = datasets.load_dataset(url, cache_dir='data')\n",
    "        \n",
    "        train_set = cls(ds['train'], train_preprocess, **kwargs)\n",
    "        val_set = cls(ds[val_split], val_preprocess, **kwargs)\n",
    "        test_set = cls(ds['test'], val_preprocess, **kwargs)\n",
    "\n",
    "        return train_set, val_set, test_set\n",
    "            \n",
    "    def __init__(self, ds, image_preprocess, tokenizer, image_length, max_length, device):\n",
    "        super().__init__()\n",
    "        self.ds = ds\n",
    "        \n",
    "        # image preprocessor\n",
    "        self.image_preprocess = image_preprocess\n",
    "        \n",
    "        # text tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        assert max_length > image_length\n",
    "        self.image_length = image_length\n",
    "        self.max_length = max_length\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        raw_image = self.ds[idx]['image']\n",
    "        question = self.ds[idx]['question']\n",
    "        answer = self.ds[idx]['answer']\n",
    "        \n",
    "        image = self.image_preprocess(raw_image)\n",
    "\n",
    "        prompt = f'Answer the following question based on the provided image.\\nQ: {question}\\nA: '\n",
    "                \n",
    "        prompt_tokens = self.tokenizer.encode(prompt)\n",
    "        answer_tokens = self.tokenizer.encode(answer)\n",
    "        \n",
    "        padded_tokens, mask = self.pad_tokens(prompt_tokens, answer_tokens)\n",
    "\n",
    "        return (\n",
    "            torch.tensor(image, device=self.device),\n",
    "            torch.tensor(padded_tokens, device=self.device),\n",
    "            torch.tensor(mask, device=self.device)\n",
    "        )\n",
    "    \n",
    "    def pad_tokens(self, prompt_tokens, answer_tokens):\n",
    "        pad = self.tokenizer.pad_token_id\n",
    "        tokens = prompt_tokens + answer_tokens\n",
    "        mask = [1 for i in prompt_tokens] + [0 for i in answer_tokens]\n",
    "        padding = self.max_length - self.image_length - len(tokens)\n",
    "        if padding > 0:\n",
    "            tokens = tokens + [pad for i in range(padding)]\n",
    "            mask = mask + [0 for i in range(padding)]\n",
    "        elif padding < 0:\n",
    "            tokens = tokens[:self.max_length - self.image_length]\n",
    "            mask = mask[:self.max_length - self.image_length]\n",
    "        return tokens, mask\n",
    "\n",
    "\n",
    "train_set, val_set, test_set = VQADataset.from_name(\n",
    "    'VQA-RAD',\n",
    "    train_preprocess=model.image_encoder.train_preprocess,\n",
    "    val_preprocess=model.image_encoder.val_preprocess,\n",
    "    tokenizer=model.text_decoder.tokenizer,\n",
    "    image_length=model.image_encoder.seq_length,\n",
    "    max_length=model.text_decoder.max_length,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "image, padded_tokens, mask = train_set[0]\n",
    "for t in train_set[0]:\n",
    "    print(t.shape, t.dtype, t.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e588b635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_30359/3239635773.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(image, device=self.device),\n",
      "/ocean/projects/asc170022p/mtragoza/mambaforge/envs/med_vqa/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1711403388920/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[29896, 29900, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896,\n",
       "         29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896,\n",
       "         29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896,\n",
       "         29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896,\n",
       "         29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896,\n",
       "         29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896,\n",
       "         29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896,\n",
       "         29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896,\n",
       "         29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896,\n",
       "         29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896,\n",
       "         29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896,\n",
       "         29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896,\n",
       "         29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896,\n",
       "         29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896,\n",
       "         29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896,\n",
       "         29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896,\n",
       "         29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896,\n",
       "         29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896,\n",
       "         29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896,\n",
       "         29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896,\n",
       "         29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896,\n",
       "         29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896,\n",
       "         29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896,\n",
       "         29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896,\n",
       "         29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896, 29896,\n",
       "         29896, 29896, 29896, 29896, 29896, 29896]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, padded_tokens, mask = train_set[0]\n",
    "output = model.generate(\n",
    "    image.unsqueeze(0), padded_tokens.unsqueeze(0), mask.unsqueeze(0), max_new_tokens=256\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ae74622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1011111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.text_decoder.tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733a43fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/449 [00:00<?, ?it/s]/var/tmp/ipykernel_30359/3239635773.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(image, device=self.device),\n",
      "loss = 18.8498:   2%|â–         | 11/449 [00:46<26:57,  3.69s/it] "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import peft\n",
    "\n",
    "peft_config = peft.LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\n",
    "        'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'lm_head'\n",
    "    ],\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM'\n",
    ")\n",
    "model.text_decoder.model = peft.get_peft_model(model.text_decoder.model, peft_config)\n",
    "\n",
    "batch_size = 4\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.text_decoder.model.parameters(), lr=1e-5)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(t:=tqdm.tqdm(train_loader)):\n",
    "        images, padded_tokens, mask = batch\n",
    "        outputs = model.forward(images, padded_tokens, mask)\n",
    "        loss = outputs.loss.detach().float()\n",
    "        t.set_description(f'loss = {loss:.4f}')\n",
    "        total_loss += loss\n",
    "        outputs.loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    total_loss /= len(train_loader)\n",
    "    print(total_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bac601",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, padded_tokens, mask = dataset[37]\n",
    "print(llama_tokenizer.decode(padded_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71217db",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(\n",
    "    image.unsqueeze(0), padded_tokens.unsqueeze(0), mask.unsqueeze(0), max_new_tokens=256\n",
    ")\n",
    "llama_tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c065e164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med_vqa",
   "language": "python",
   "name": "med_vqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
